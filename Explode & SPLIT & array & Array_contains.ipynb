{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e22a3ea1-bf05-459d-8872-29bed9ecfdc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import explode,col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54b183b7-09b7-4477-bec4-d14e1e5bc499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>skills</th></tr></thead><tbody><tr><td>1</td><td>Ram</td><td>List(python, spark)</td></tr><tr><td>2</td><td>Sham</td><td>List(java, sql)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Ram",
         [
          "python",
          "spark"
         ]
        ],
        [
         2,
         "Sham",
         [
          "java",
          "sql"
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "skills",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- skills: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "data = [(1,\"Ram\",['python','spark']),(2,'Sham',['java','sql'])]\n",
    "schema =['id','name','skills']\n",
    "\n",
    "df=spark.createDataFrame(data,schema)\n",
    "display(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0d7921a-3e34-4f6e-a361-10f9bc3ce724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------------+\n| id|name|         skills|\n+---+----+---------------+\n|  1| Ram|[python, spark]|\n|  2|Sham|    [java, sql]|\n+---+----+---------------+\n\n+---+----+---------------+------+\n| id|name|         skills| skill|\n+---+----+---------------+------+\n|  1| Ram|[python, spark]|python|\n|  1| Ram|[python, spark]| spark|\n|  2|Sham|    [java, sql]|  java|\n|  2|Sham|    [java, sql]|   sql|\n+---+----+---------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# It will create a new row for each element in a array column. \n",
    "df.show()\n",
    "df1=df.withColumn('skill',explode(col('skills')))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a1f4035-9f8b-4a9b-9ae2-6822bd2b36b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------------+\n|id |name     |name_split  |\n+---+---------+------------+\n|1  |Saket,Jha|[Saket, Jha]|\n|2  |Anuj,Jha |[Anuj, Jha] |\n+---+---------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "\n",
    "data = [(1, \"Saket,Jha\"), (2, \"Anuj,Jha\")]\n",
    "columns = [\"id\", \"name\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df = df.withColumn(\"name_split\", split(df[\"name\"], \",\"))\n",
    "\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a94c7ccb-e876-40f7-882d-c520744d1101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------------+----------+---------+\n|id |name     |name_split  |first_name|last_name|\n+---+---------+------------+----------+---------+\n|1  |Saket,Jha|[Saket, Jha]|Saket     |Jha      |\n|2  |Anuj,Jha |[Anuj, Jha] |Anuj      |Jha      |\n+---+---------+------------+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"first_name\", col(\"name_split\")[0]) \\\n",
    "       .withColumn(\"last_name\", col(\"name_split\")[1])\n",
    "\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed38dc28-a00c-4666-b428-06e9315cecc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+\n|name |age|info       |\n+-----+---+-----------+\n|Anuj |25 |[Anuj, 25] |\n|panki|30 |[panki, 30]|\n+-----+---+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array, lit\n",
    "\n",
    "data = [(\"Anuj\", 25), (\"panki\", 30)]\n",
    "columns = [\"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df = df.withColumn(\"info\", array(df[\"name\"], df[\"age\"])) # we are adding an array col name.\n",
    "\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54aca68e-319f-42c0-bb79-df68c0d8d35d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+--------------------+\n|name |age|info       |hobbies             |\n+-----+---+-----------+--------------------+\n|Anuj |25 |[Anuj, 25] |[Reading, Traveling]|\n|panki|30 |[panki, 30]|[Reading, Traveling]|\n+-----+---+-----------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"hobbies\", array(lit(\"Reading\"), lit(\"Traveling\")))\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3afb0a2a-ded2-4fca-b251-d5a5e5d96559",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n|name |hobby    |\n+-----+---------+\n|Anuj |Reading  |\n|Anuj |Traveling|\n|panki|Reading  |\n|panki|Traveling|\n+-----+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "#using Explode, alias with array\n",
    "df_exploded = df.select(\"name\", explode(df[\"hobbies\"]).alias(\"hobby\"))\n",
    "df_exploded.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21ab4d29-1a78-442b-80d0-22a395dc86a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Explode & SPLIT & array & Array_contains",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
