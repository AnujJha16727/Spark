{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff97c619-c966-4262-a307-0b0caeae9ed6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+----+\n| id| name|gender|salary|dept|\n+---+-----+------+------+----+\n|  1|Saket|  male|  2000|   1|\n|  2|Anuja|female|  4000|   1|\n|  3| Anuj|  male|  3000|   2|\n|  4|  Anu|female|  2000|   4|\n+---+-----+------+------+----+\n\n+---+----+\n| id|name|\n+---+----+\n|  1|  IT|\n|  2|  HR|\n|  3| OPS|\n+---+----+\n\n"
     ]
    }
   ],
   "source": [
    "data1 = [\n",
    "    (1, 'Saket', 'male', 2000,1),\n",
    "    (2, 'Anuja', 'female', 4000, 1),\n",
    "    (3, 'Anuj', 'male', 3000, 2),  \n",
    "    (4, 'Anu', 'female', 2000, 4)\n",
    "]\n",
    "schema1 = ['id', 'name', 'gender', 'salary','dept']\n",
    "data2 = [(1,'IT'),(2,'HR'),(3,'OPS')]\n",
    "schema2=['id','name']\n",
    "\n",
    "empDf = spark.createDataFrame(data1, schema1)\n",
    "empDf.show()\n",
    "\n",
    "deptDf = spark.createDataFrame(data2, schema2)\n",
    "deptDf.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07ee255a-ebba-49ae-a91d-2c549d44cc7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method join in module pyspark.sql.dataframe:\n\njoin(other: 'DataFrame', on: Union[str, List[str], pyspark.sql.column.Column, List[pyspark.sql.column.Column], NoneType] = None, how: Optional[str] = None) -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n    Joins with another :class:`DataFrame`, using the given join expression.\n    \n    .. versionadded:: 1.3.0\n    \n    .. versionchanged:: 3.4.0\n        Support Spark Connect.\n    \n    Parameters\n    ----------\n    other : :class:`DataFrame`\n        Right side of the join\n    on : str, list or :class:`Column`, optional\n        a string for the join column name, a list of column names,\n        a join expression (Column), or a list of Columns.\n        If `on` is a string or a list of strings indicating the name of the join column(s),\n        the column(s) must exist on both sides, and this performs an equi-join.\n    how : str, optional\n        default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n        ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n        ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n        ``anti``, ``leftanti`` and ``left_anti``.\n    \n    Returns\n    -------\n    :class:`DataFrame`\n        Joined DataFrame.\n    \n    Examples\n    --------\n    The following performs a full outer join between ``df1`` and ``df2``.\n    \n    >>> from pyspark.sql import Row\n    >>> from pyspark.sql.functions import desc\n    >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")]).toDF(\"age\", \"name\")\n    >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n    >>> df3 = spark.createDataFrame([Row(age=2, name=\"Alice\"), Row(age=5, name=\"Bob\")])\n    >>> df4 = spark.createDataFrame([\n    ...     Row(age=10, height=80, name=\"Alice\"),\n    ...     Row(age=5, height=None, name=\"Bob\"),\n    ...     Row(age=None, height=None, name=\"Tom\"),\n    ...     Row(age=None, height=None, name=None),\n    ... ])\n    \n    Inner join on columns (default)\n    \n    >>> df.join(df2, 'name').select(df.name, df2.height).show()\n    +----+------+\n    |name|height|\n    +----+------+\n    | Bob|    85|\n    +----+------+\n    >>> df.join(df4, ['name', 'age']).select(df.name, df.age).show()\n    +----+---+\n    |name|age|\n    +----+---+\n    | Bob|  5|\n    +----+---+\n    \n    Outer join for both DataFrames on the 'name' column.\n    \n    >>> df.join(df2, df.name == df2.name, 'outer').select(\n    ...     df.name, df2.height).sort(desc(\"name\")).show()\n    +-----+------+\n    | name|height|\n    +-----+------+\n    |  Bob|    85|\n    |Alice|  null|\n    | null|    80|\n    +-----+------+\n    >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).show()\n    +-----+------+\n    | name|height|\n    +-----+------+\n    |  Tom|    80|\n    |  Bob|    85|\n    |Alice|  null|\n    +-----+------+\n    \n    Outer join for both DataFrams with multiple columns.\n    \n    >>> df.join(\n    ...     df3,\n    ...     [df.name == df3.name, df.age == df3.age],\n    ...     'outer'\n    ... ).select(df.name, df3.age).show()\n    +-----+---+\n    | name|age|\n    +-----+---+\n    |Alice|  2|\n    |  Bob|  5|\n    +-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "help(empDf.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bd3c81c-557f-430d-8845-6dc4779c22c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+----+---+----+\n| id| name|gender|salary|dept| id|name|\n+---+-----+------+------+----+---+----+\n|  1|Saket|  male|  2000|   1|  1|  IT|\n|  2|Anuja|female|  4000|   1|  1|  IT|\n|  3| Anuj|  male|  3000|   2|  2|  HR|\n+---+-----+------+------+----+---+----+\n\n"
     ]
    }
   ],
   "source": [
    "empDf.join(deptDf, empDf.dept==deptDf.id, 'inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4558716a-3d79-4f52-aa8f-8231cfef9c8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+----+----+----+\n| id| name|gender|salary|dept|  id|name|\n+---+-----+------+------+----+----+----+\n|  1|Saket|  male|  2000|   1|   1|  IT|\n|  2|Anuja|female|  4000|   1|   1|  IT|\n|  3| Anuj|  male|  3000|   2|   2|  HR|\n|  4|  Anu|female|  2000|   4|null|null|\n+---+-----+------+------+----+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "empDf.join(deptDf, empDf.dept==deptDf.id, 'left').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dd1fe5e-dc62-4ed4-9f15-080f99ea713e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+------+----+---+----+\n|  id| name|gender|salary|dept| id|name|\n+----+-----+------+------+----+---+----+\n|   2|Anuja|female|  4000|   1|  1|  IT|\n|   1|Saket|  male|  2000|   1|  1|  IT|\n|   3| Anuj|  male|  3000|   2|  2|  HR|\n|null| null|  null|  null|null|  3| OPS|\n+----+-----+------+------+----+---+----+\n\n"
     ]
    }
   ],
   "source": [
    "empDf.join(deptDf, empDf.dept==deptDf.id, 'right').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93c30cb6-605f-4ceb-a673-5bc8cb14a979",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+------+----+----+----+\n|  id| name|gender|salary|dept|  id|name|\n+----+-----+------+------+----+----+----+\n|   1|Saket|  male|  2000|   1|   1|  IT|\n|   2|Anuja|female|  4000|   1|   1|  IT|\n|   3| Anuj|  male|  3000|   2|   2|  HR|\n|null| null|  null|  null|null|   3| OPS|\n|   4|  Anu|female|  2000|   4|null|null|\n+----+-----+------+------+----+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "empDf.join(deptDf, empDf.dept==deptDf.id, 'Full').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5462a7bf-f63a-4496-80d6-3b5a3efc57df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+----+\n| id| name|gender|salary|dept|\n+---+-----+------+------+----+\n|  1|Saket|  male|  2000|   1|\n|  2|Anuja|female|  4000|   1|\n|  3| Anuj|  male|  3000|   2|\n+---+-----+------+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "#left Semi join\n",
    "empDf.join(deptDf, empDf.dept==deptDf.id, 'leftsemi').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e877945a-f8c4-4bab-b6ca-f54b57228b76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+------+----+\n| id|name|gender|salary|dept|\n+---+----+------+------+----+\n|  4| Anu|female|  2000|   4|\n+---+----+------+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "empDf.join(deptDf, empDf.dept==deptDf.id, 'leftanti').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5a2e521-2881-42d9-b364-cceba0979c93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Join",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
