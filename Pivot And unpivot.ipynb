{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b5f249c-3f2e-4557-8c01-13df5ae7dad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n|  Name|Month|Sales|\n+------+-----+-----+\n|  Anuj|  Jan|  200|\n|  Anuj|  Feb|  250|\n|   Bob|  Jan|  300|\n|   Bob|  Feb|  350|\n|Marley|  Jan|  400|\n|Marley|  Feb|  450|\n+------+-----+-----+\n\n+------+---+---+\n|  Name|Feb|Jan|\n+------+---+---+\n|  Anuj|250|200|\n|   Bob|350|300|\n|Marley|450|400|\n+------+---+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "data = [\n",
    "    (\"Anuj\", \"Jan\", 200),\n",
    "    (\"Anuj\", \"Feb\", 250),\n",
    "    (\"Bob\", \"Jan\", 300),\n",
    "    (\"Bob\", \"Feb\", 350),\n",
    "    (\"Marley\", \"Jan\", 400),\n",
    "    (\"Marley\", \"Feb\", 450)\n",
    "]\n",
    "\n",
    "columns = [\"Name\", \"Month\", \"Sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "df_pivot = df.groupBy(\"Name\").pivot(\"Month\").agg(sum(\"Sales\"))#by the use of pivot we can convert a single column data into multiple column data or say different rows are converted into different column.\n",
    "\n",
    "df_pivot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61101d09-8073-45e3-9e02-ce31fb0f59ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n|  Name|Month|Sales|\n+------+-----+-----+\n|  Anuj|  Jan|  200|\n|  Anuj|  Feb|  250|\n|   Bob|  Jan|  300|\n|   Bob|  Feb|  350|\n|Marley|  Jan|  400|\n|Marley|  Feb|  450|\n+------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df_unpivot = df.selectExpr(\n",
    "    \"Name\",\n",
    "    \"Month\",\n",
    "    \"Sales\"\n",
    ").filter(\"Month IN ('Jan', 'Feb')\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "569b816b-4db8-47e4-a21a-0b5e17a95f55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+\n|  Name|Feb|Jan|\n+------+---+---+\n|  Anuj|250|200|\n|   Bob|350|300|\n|Marley|450|400|\n+------+---+---+\n\n"
     ]
    }
   ],
   "source": [
    "df_pivot.fillna(0).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9d1a93d-4016-47ea-9161-266d5d564cdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pivot And unpivot",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
